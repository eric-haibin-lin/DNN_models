{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import sys\n",
    "import networkx as nx\n",
    "import igraph as ig\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoderModel:\n",
    "    def __init__(self, dims, internal_act=None, output_act=None):\n",
    "        self.data = mx.symbol.Variable('data')\n",
    "        self.y = mx.symbol.Variable('label')\n",
    "        self.fc1_weight = mx.symbol.Variable('fc1_weight')\n",
    "        self.fc1_bias = mx.symbol.Variable('fc1_bias')\n",
    "        self.fc2_weight = mx.symbol.Variable('fc2_weight')\n",
    "        self.fc2_bias = mx.symbol.Variable('fc2_bias')\n",
    "        x = mx.symbol.FullyConnected(data=self.data, weight=self.fc1_weight,\n",
    "                                     bias=self.fc1_bias, num_hidden=dims[1])\n",
    "        if (internal_act is not None):\n",
    "            x = mx.symbol.Activation(data=x, act_type=internal_act)\n",
    "            print(\"Internal activation: \" + internal_act)\n",
    "        self.layer1 = x\n",
    "        x = mx.symbol.FullyConnected(data=x, weight=self.fc2_weight,\n",
    "                                     bias=self.fc2_bias, num_hidden=dims[2])\n",
    "        if (output_act is not None):\n",
    "            x = mx.symbol.Activation(data=x, act_type=output_act)\n",
    "            print(\"Output activation: \" + output_act)\n",
    "        self.layer2 = x\n",
    "        self.loss = mx.symbol.LinearRegressionOutput(data=x, label=self.y)\n",
    "        self.model = mx.mod.Module(symbol=self.loss, data_names=['data'], label_names = ['label'])\n",
    "\n",
    "    def fit(self, data, batch_size, num_epoch, params=None, learning_rate=0.005, reinit_opt=True):\n",
    "        data_iter = mx.io.NDArrayIter(data={'data':data}, label={'label':data},\n",
    "                batch_size=batch_size, shuffle=True,\n",
    "                last_batch_handle='roll_over')\n",
    "        \n",
    "        if (params is None):\n",
    "            print(\"Learning rate: \" + str(learning_rate))\n",
    "            print(\"batch size: \" + str(batch_size))\n",
    "            print(\"internal #epochs: \" + str(num_epoch))\n",
    "            # allocate memory given the input data and label shapes\n",
    "            self.model.bind(data_shapes=data_iter.provide_data, label_shapes=data_iter.provide_label)\n",
    "            # initialize parameters by uniform random numbers\n",
    "            self.model.init_params(initializer=mx.init.Uniform(scale=.1))\n",
    "            # use SGD with learning rate 0.1 to train\n",
    "            self.model.init_optimizer(optimizer='sgd',\n",
    "                                      optimizer_params={'learning_rate': learning_rate,\n",
    "                                                        'momentum': 0.9})\n",
    "        else:\n",
    "            self.model.set_params(arg_params=params, aux_params=None, force_init=True)\n",
    "            if (reinit_opt):\n",
    "                print(\"reinit optimizer. New learning rate: \" + str(learning_rate))\n",
    "                self.model.init_optimizer(optimizer='sgd',\n",
    "                                          optimizer_params={'learning_rate': learning_rate,\n",
    "                                                            'momentum': 0.9}, force_init=True)\n",
    "        # use accuracy as the metric\n",
    "        metric = mx.metric.create('acc')\n",
    "        # train 5 epochs, i.e. going over the data iter one pass\n",
    "        for epoch in range(num_epoch):\n",
    "            data_iter.reset()\n",
    "            metric.reset()\n",
    "            for batch in data_iter:\n",
    "                self.model.forward(batch, is_train=True)       # compute predictions\n",
    "                self.model.update_metric(metric, batch.label)  # accumulate prediction accuracy\n",
    "                self.model.backward()                          # compute gradients\n",
    "                self.model.update()                            # update parameters\n",
    "            #print('Epoch %d, Training %s' % (epoch, metric.get()))\n",
    "        #self.model.fit(data_iter, optimizer_params={'learning_rate':learning_rate, 'momentum': 0.9},\n",
    "        #        optimizer='sgd', num_epoch=50, eval_metric='mse', force_rebind=True,\n",
    "        #        batch_end_callback = mx.callback.Speedometer(batch_size, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data, num_dims, num_epoc, internal_act=None, output_act=None, learning_rate=0.005, batch_size=50):\n",
    "    int_epoc = 100\n",
    "    params = None\n",
    "    model = AutoEncoderModel([data.shape[1], num_dims, data.shape[1]],\n",
    "                             internal_act, output_act)\n",
    "    prev_val = None\n",
    "    reinit_opt = True\n",
    "    for i in range(num_epoc/int_epoc):\n",
    "        model.fit(data, batch_size, int_epoc, params, learning_rate, reinit_opt=reinit_opt)\n",
    "        reinit_opt = False\n",
    "\n",
    "        params = model.model.get_params()[0]\n",
    "        fc1_weight = params.get('fc1_weight').asnumpy()\n",
    "        fc1_bias = params.get('fc1_bias').asnumpy()\n",
    "        fc2_weight = params.get('fc2_weight').asnumpy()\n",
    "        fc2_bias = params.get('fc2_bias').asnumpy()\n",
    "\n",
    "        np_data = data.asnumpy()\n",
    "        hidden = np.dot(np_data, fc1_weight.T) + fc1_bias\n",
    "        output = np.dot(hidden, fc2_weight.T) + fc2_bias\n",
    "        val = np.sum(np.square(output - np_data))\n",
    "        print(\"epoc \" + str(i * int_epoc) + \": \" + str(val))\n",
    "        if (prev_val is not None and prev_val < val):\n",
    "            learning_rate = learning_rate / 2\n",
    "            reinit_opt = True\n",
    "        prev_val = val\n",
    "        sys.stdout.flush()\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run on a low-rank data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max: \n",
      "[ 6.10102654]\n",
      "<NDArray 1 @cpu(0)>\n",
      "(1000L, 100L)\n"
     ]
    }
   ],
   "source": [
    "rand_data1 = mx.ndarray.random_uniform(shape=[1000, 10])\n",
    "rand_data2 = mx.ndarray.random_uniform(shape=[10, 100])\n",
    "rand_data = mx.ndarray.dot(rand_data1, rand_data2)\n",
    "print(\"max: \" + str(mx.ndarray.max(rand_data)))\n",
    "rand_data = rand_data / mx.ndarray.max(rand_data)\n",
    "print(rand_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.01955516  0.03985536 -0.07798745 ..., -0.05676827 -0.22096498\n",
      "  -4.35739613]\n",
      " [ 0.06220453  0.07652628  0.03788156 ...,  0.16164172  0.0135784\n",
      "  -3.72151923]\n",
      " [-0.0311646   0.03257374 -0.3144455  ..., -0.03104994  0.25370428\n",
      "  -4.25794554]\n",
      " ..., \n",
      " [-0.02363796 -0.00566644 -0.08939761 ...,  0.00696465 -0.15669249\n",
      "  -3.04979634]\n",
      " [-0.0161101  -0.05258452 -0.21494456 ...,  0.14610089 -0.06200607\n",
      "  -3.78889537]\n",
      " [ 0.1601655   0.02495101 -0.0750294  ...,  0.10465452  0.2186821\n",
      "  -2.50724936]]\n",
      "502.355621757\n",
      "-4607.53736694\n",
      "svd error: 1.68328e-08\n"
     ]
    }
   ],
   "source": [
    "np_rand_data = rand_data.asnumpy()\n",
    "U, s, Vh = sp.sparse.linalg.svds(np_rand_data, k=10)\n",
    "low_dim_data = np.dot(np_rand_data, Vh.T)\n",
    "print(low_dim_data)\n",
    "print(sum(low_dim_data[low_dim_data > 0]))\n",
    "print(sum(low_dim_data[low_dim_data < 0]))\n",
    "res = np.dot(low_dim_data, Vh)\n",
    "print(\"svd error: \" + str(np.sum(np.square(res - np_rand_data))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.2\n",
      "batch size: 50\n",
      "internal #epochs: 100\n",
      "epoc 0: 157.689\n",
      "epoc 100: 120.806\n",
      "epoc 200: 76.3864\n",
      "epoc 300: 48.0992\n",
      "epoc 400: 30.3625\n",
      "epoc 500: 17.6435\n",
      "epoc 600: 9.88616\n",
      "epoc 700: 5.48403\n",
      "epoc 800: 2.71852\n",
      "epoc 900: 1.1848\n",
      "epoc 1000: 0.489893\n",
      "epoc 1100: 0.204632\n",
      "epoc 1200: 0.088024\n",
      "epoc 1300: 0.0387866\n",
      "epoc 1400: 0.0173438\n",
      "epoc 1500: 0.00782017\n",
      "epoc 1600: 0.00353995\n",
      "epoc 1700: 0.00160629\n",
      "epoc 1800: 0.000729828\n",
      "epoc 1900: 0.000331956\n",
      "epoc 2000: 0.00015116\n",
      "epoc 2100: 6.90351e-05\n",
      "epoc 2200: 3.17257e-05\n",
      "epoc 2300: 1.48614e-05\n",
      "epoc 2400: 7.34132e-06\n",
      "epoc 2500: 4.08044e-06\n",
      "epoc 2600: 2.64298e-06\n",
      "epoc 2700: 2.08031e-06\n",
      "epoc 2800: 1.73534e-06\n",
      "epoc 2900: 1.63155e-06\n",
      "epoc 3000: 1.55802e-06\n",
      "epoc 3100: 1.51333e-06\n",
      "epoc 3200: 1.48502e-06\n",
      "epoc 3300: 1.41454e-06\n",
      "epoc 3400: 1.40252e-06\n",
      "epoc 3500: 1.37172e-06\n",
      "epoc 3600: 1.357e-06\n",
      "epoc 3700: 1.34649e-06\n",
      "epoc 3800: 1.34212e-06\n",
      "epoc 3900: 1.33692e-06\n",
      "epoc 4000: 1.32821e-06\n",
      "epoc 4100: 1.32531e-06\n",
      "epoc 4200: 1.3017e-06\n",
      "epoc 4300: 1.29849e-06\n",
      "epoc 4400: 1.29058e-06\n",
      "epoc 4500: 1.27335e-06\n",
      "epoc 4600: 1.27022e-06\n",
      "epoc 4700: 1.26511e-06\n",
      "epoc 4800: 1.26322e-06\n",
      "epoc 4900: 1.26032e-06\n"
     ]
    }
   ],
   "source": [
    "params_linear_r10=train(rand_data, 10, 5000, learning_rate=0.2, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Internal activation: tanh\n",
      "Learning rate: 0.0005\n",
      "batch size: 100\n",
      "internal #epochs: 100\n",
      "epoc 0: 14712.6\n",
      "epoc 100: 10943.1\n",
      "epoc 200: 6606.42\n",
      "epoc 300: 3053.52\n",
      "epoc 400: 1170.41\n",
      "epoc 500: 740.22\n",
      "epoc 600: 1099.36\n",
      "reinit optimizer. New learning rate: 0.00025\n",
      "epoc 700: 1392.52\n",
      "reinit optimizer. New learning rate: 0.000125\n",
      "epoc 800: 1546.96\n",
      "reinit optimizer. New learning rate: 6.25e-05\n",
      "epoc 900: 1624.36\n",
      "reinit optimizer. New learning rate: 3.125e-05\n",
      "epoc 1000: 1662.95\n",
      "reinit optimizer. New learning rate: 1.5625e-05\n",
      "epoc 1100: 1682.19\n",
      "reinit optimizer. New learning rate: 7.8125e-06\n",
      "epoc 1200: 1691.79\n",
      "reinit optimizer. New learning rate: 3.90625e-06\n",
      "epoc 1300: 1696.7\n",
      "reinit optimizer. New learning rate: 1.953125e-06\n",
      "epoc 1400: 1698.88\n",
      "reinit optimizer. New learning rate: 9.765625e-07\n",
      "epoc 1500: 1699.7\n",
      "reinit optimizer. New learning rate: 4.8828125e-07\n",
      "epoc 1600: 1699.94\n",
      "reinit optimizer. New learning rate: 2.44140625e-07\n",
      "epoc 1700: 1699.99\n",
      "reinit optimizer. New learning rate: 1.220703125e-07\n",
      "epoc 1800: 1700.01\n",
      "reinit optimizer. New learning rate: 6.103515625e-08\n",
      "epoc 1900: 1700.01\n",
      "reinit optimizer. New learning rate: 3.0517578125e-08\n",
      "epoc 2000: 1700.01\n",
      "reinit optimizer. New learning rate: 1.52587890625e-08\n",
      "epoc 2100: 1700.01\n",
      "reinit optimizer. New learning rate: 7.62939453125e-09\n",
      "epoc 2200: 1700.01\n",
      "epoc 2300: 1700.01\n",
      "epoc 2400: 1700.01\n",
      "reinit optimizer. New learning rate: 3.81469726563e-09\n",
      "epoc 2500: 1700.01\n",
      "epoc 2600: 1700.01\n",
      "epoc 2700: 1700.01\n",
      "epoc 2800: 1700.01\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-108-15426ec593cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mparams_sigmoid_r10\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrand_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minternal_act\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tanh'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0005\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-103-405aee1c33fa>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(data, num_dims, num_epoc, internal_act, output_act, learning_rate, batch_size)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mreinit_opt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epoc\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mint_epoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint_epoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreinit_opt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreinit_opt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mreinit_opt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-102-b86b6aa496fa>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, data, batch_size, num_epoch, params, learning_rate, reinit_opt)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# accumulate prediction accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m                          \u001b[0;31m# compute gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m                            \u001b[0;31m# update parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m             \u001b[0;31m#print('Epoch %d, Training %s' % (epoch, metric.get()))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;31m#self.model.fit(data_iter, optimizer_params={'learning_rate':learning_rate, 'momentum': 0.9},\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/mxnet-0.12.0-py2.7.egg/mxnet/module/module.pyc\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    634\u001b[0m                            \u001b[0mnum_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m                            \u001b[0mkvstore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_kvstore\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 636\u001b[0;31m                            param_names=self._exec_group.param_names)\n\u001b[0m\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerge_multi_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/mxnet-0.12.0-py2.7.egg/mxnet/model.pyc\u001b[0m in \u001b[0;36m_update_params\u001b[0;34m(param_arrays, grad_arrays, updater, num_device, kvstore, param_names)\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0;31m# use a better solution later\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0mupdater\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnum_device\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/mxnet-0.12.0-py2.7.egg/mxnet/optimizer.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, index, grad, weight)\u001b[0m\n\u001b[1;32m   1141\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_state_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstates_synced\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1143\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_multi_precision\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msync_state_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/mxnet-0.12.0-py2.7.egg/mxnet/optimizer.pyc\u001b[0m in \u001b[0;36mupdate_multi_precision\u001b[0;34m(self, index, weight, grad, state)\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0muse_multi_precision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulti_precision\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         self._update_impl(index, weight, grad, state,\n\u001b[0;32m--> 525\u001b[0;31m                           multi_precision=use_multi_precision)\n\u001b[0m\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m \u001b[0;31m# pylint: enable=line-too-long\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/mxnet-0.12.0-py2.7.egg/mxnet/optimizer.pyc\u001b[0m in \u001b[0;36m_update_impl\u001b[0;34m(self, index, weight, grad, state, multi_precision)\u001b[0m\n\u001b[1;32m    505\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m                 sgd_mom_update(weight, grad, state, out=weight,\n\u001b[0;32m--> 507\u001b[0;31m                                lr=lr, wd=wd, **kwargs)\n\u001b[0m\u001b[1;32m    508\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m                 sgd_update(weight, grad, out=weight,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/mxnet-0.12.0-py2.7.egg/mxnet/ndarray/register.pyc\u001b[0m in \u001b[0;36msgd_mom_update\u001b[0;34m(weight, grad, mom, lr, momentum, wd, rescale_grad, clip_gradient, out, name, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/mxnet-0.12.0-py2.7.egg/mxnet/_ctypes/ndarray.pyc\u001b[0m in \u001b[0;36m_imperative_invoke\u001b[0;34m(handle, ndargs, keys, vals, out)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mc_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_char_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mc_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mc_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_char_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mc_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         ctypes.byref(out_stypes)))\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0moriginal_output\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "params_sigmoid_r10=train(rand_data, 10, 5000, internal_act='sigmoid', learning_rate=0.0005, batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run on real data\n",
    "\n",
    "We compute the embedding on a graph with 81306 vertices and 1768149 vertices. To embed the graph into 10 dimensions, we start with the most densest columns and increase the number of columns to embed. When we increase the number of columns to embed, we use the parameters trained from the previous run (on the dataset with a smaller number of columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "elg = nx.read_edgelist(\"/home/ubuntu/datasets/twitter_combined.txt\")\n",
    "spm = nx.to_scipy_sparse_matrix(elg, dtype='f')\n",
    "colsum = np.ravel(spm.sum(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the embedding on the densest 10 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ 2490.  3383.  2484.  2758.  2476.  1789.  2133.  3011.  3239.  2155.]\n",
      "<NDArray 10 @cpu(0)>\n",
      "(81306L, 10L)\n"
     ]
    }
   ],
   "source": [
    "max10 = np.sort(np.ravel(colsum), axis=None)[len(colsum) - 10]\n",
    "data10 = mx.ndarray.sparse.csr_matrix(spm[:,colsum >= max10])\n",
    "print(mx.ndarray.sum(data10, axis=0))\n",
    "print(data10.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(81306, 10)\n",
      "1.21314\n",
      "-2.3992\n",
      "svd error: 6.78263e-09\n"
     ]
    }
   ],
   "source": [
    "np_data10 = data10.asnumpy()\n",
    "U, s, Vh = sp.linalg.svd(np_data10, full_matrices=False)\n",
    "low_dim_data = np.dot(np_data10, Vh.T)\n",
    "print(low_dim_data.shape)\n",
    "print(np.max(low_dim_data))\n",
    "print(np.min(low_dim_data))\n",
    "res = np.dot(low_dim_data, Vh)\n",
    "print(\"svd error: \" + str(np.sum(np.square(res - np_data10))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.005\n",
      "epoc 0: 3479.27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:optimizer already initialized, ignoring...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.005\n",
      "epoc 100: 1024.56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:optimizer already initialized, ignoring...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.005\n",
      "epoc 200: 831.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:optimizer already initialized, ignoring...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.005\n",
      "epoc 300: 574.544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:optimizer already initialized, ignoring...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.005\n",
      "epoc 400: 356.311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:optimizer already initialized, ignoring...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.005\n",
      "epoc 500: 273.701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:optimizer already initialized, ignoring...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.005\n",
      "epoc 600: 157.228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:optimizer already initialized, ignoring...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.005\n",
      "epoc 700: 99.5471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:optimizer already initialized, ignoring...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.005\n",
      "epoc 800: 90.7048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:optimizer already initialized, ignoring...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.005\n",
      "epoc 900: 89.9733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:optimizer already initialized, ignoring...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.005\n",
      "epoc 1000: 89.8885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:optimizer already initialized, ignoring...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.005\n",
      "epoc 1100: 89.8341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:optimizer already initialized, ignoring...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.005\n",
      "epoc 1200: 89.7637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:optimizer already initialized, ignoring...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.005\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-17d0f88ed619>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mparams_linear10\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minternal_act\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-afcc28db7e52>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(data, num_dims, num_epoc, internal_act, output_act, learning_rate)\u001b[0m\n\u001b[1;32m      5\u001b[0m                              internal_act, output_act)\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epoc\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mint_epoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint_epoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-3812f03a41df>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, data, batch_size, params, learning_rate)\u001b[0m\n\u001b[1;32m     31\u001b[0m         self.model.fit(data_iter, optimizer_params={'learning_rate':learning_rate, 'momentum': 0.9},\n\u001b[1;32m     32\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sgd'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_metric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mse'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_rebind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                 batch_end_callback = mx.callback.Speedometer(batch_size, 2))\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/mxnet-0.12.0-py2.7.egg/mxnet/module/base_module.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_data, eval_data, eval_metric, epoch_end_callback, batch_end_callback, kvstore, optimizer, optimizer_params, eval_end_callback, eval_batch_end_callback, initializer, arg_params, aux_params, allow_missing, force_rebind, force_init, begin_epoch, num_epoch, validation_metric, monitor)\u001b[0m\n\u001b[1;32m    486\u001b[0m                     \u001b[0mmonitor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m                     \u001b[0;31m# pre fetch next batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/mxnet-0.12.0-py2.7.egg/mxnet/module/module.pyc\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    634\u001b[0m                            \u001b[0mnum_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m                            \u001b[0mkvstore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_kvstore\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 636\u001b[0;31m                            param_names=self._exec_group.param_names)\n\u001b[0m\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerge_multi_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/mxnet-0.12.0-py2.7.egg/mxnet/model.pyc\u001b[0m in \u001b[0;36m_update_params\u001b[0;34m(param_arrays, grad_arrays, updater, num_device, kvstore, param_names)\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0;31m# use a better solution later\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0mupdater\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnum_device\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/mxnet-0.12.0-py2.7.egg/mxnet/optimizer.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, index, grad, weight)\u001b[0m\n\u001b[1;32m   1141\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_state_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstates_synced\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1143\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_multi_precision\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msync_state_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/mxnet-0.12.0-py2.7.egg/mxnet/optimizer.pyc\u001b[0m in \u001b[0;36mupdate_multi_precision\u001b[0;34m(self, index, weight, grad, state)\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0muse_multi_precision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulti_precision\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         self._update_impl(index, weight, grad, state,\n\u001b[0;32m--> 525\u001b[0;31m                           multi_precision=use_multi_precision)\n\u001b[0m\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m \u001b[0;31m# pylint: enable=line-too-long\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/mxnet-0.12.0-py2.7.egg/mxnet/optimizer.pyc\u001b[0m in \u001b[0;36m_update_impl\u001b[0;34m(self, index, weight, grad, state, multi_precision)\u001b[0m\n\u001b[1;32m    505\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m                 sgd_mom_update(weight, grad, state, out=weight,\n\u001b[0;32m--> 507\u001b[0;31m                                lr=lr, wd=wd, **kwargs)\n\u001b[0m\u001b[1;32m    508\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m                 sgd_update(weight, grad, out=weight,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/mxnet-0.12.0-py2.7.egg/mxnet/ndarray/register.pyc\u001b[0m in \u001b[0;36msgd_mom_update\u001b[0;34m(weight, grad, mom, lr, momentum, wd, rescale_grad, clip_gradient, out, name, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/mxnet-0.12.0-py2.7.egg/mxnet/_ctypes/ndarray.pyc\u001b[0m in \u001b[0;36m_imperative_invoke\u001b[0;34m(handle, ndargs, keys, vals, out)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mc_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_char_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mc_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mc_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_char_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mc_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         ctypes.byref(out_stypes)))\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0moriginal_output\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "params_linear10=train(data10, 10, 5000, internal_act=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Internal activation: tanh\n",
      "Learning rate: 0.0001\n",
      "batch size: 50\n",
      "internal #epochs: 100\n",
      "epoc 0: 18296.1\n"
     ]
    }
   ],
   "source": [
    "params_sigmoid10=train(data10, 10, 5000, internal_act='tanh', learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params_relu10=train(data10, 10, 5000, internal_act='relu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the embedding on the densest 30 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max30 = np.sort(np.ravel(colsum), axis=None)[len(colsum) - 30]\n",
    "sp_data30 = spm[:,colsum >= max30]\n",
    "data30 = mx.ndarray.sparse.csr_matrix(sp_data30)\n",
    "print(mx.ndarray.sum(data30, axis=0))\n",
    "print(data30.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to start with SVD and see how well it performs.\n",
    "One question we need to address is **what is the advantage of autoencoder over SVD**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, s, Vh = sp.sparse.linalg.svds(sp_data30, k=10)\n",
    "res = np.dot(sp_data30.dot(Vh.T), Vh)\n",
    "print(\"svd error: \" + str(np.sum(np.square(res - sp_data30))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_linear30=train(data30, 5000, internal_act=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params_linear30=train(data30, 5000, internal_act='sigmoid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the embedding on the densest 1000 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max1000 = np.sort(np.ravel(colsum), axis=None)[len(colsum) - 1000]\n",
    "sp_data1000 = spm[:,colsum >= max1000]\n",
    "data1000 = mx.ndarray.sparse.csr_matrix(sp_data1000)\n",
    "print(mx.ndarray.sum(data1000, axis=0))\n",
    "print(data1000.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, s, Vh = sp.sparse.linalg.svds(sp_data1000, k=100)\n",
    "res = np.dot(sp_data1000.dot(Vh.T), Vh)\n",
    "print(\"svd error: \" + str(np.sum(np.square(res - sp_data1000))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_linear1000=train(data1000, num_dims=100, num_epoc=5000, internal_act=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pref_matrix = [[0.9, 0.1], [0.1, 0.9]]\n",
    "block_sizes = [70, 30]\n",
    "g = ig.Graph.SBM(100, pref_matrix, block_sizes, directed=True)\n",
    "sim_spm = g.get_adjacency()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

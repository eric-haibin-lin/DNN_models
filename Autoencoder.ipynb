{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import sys\n",
    "import networkx as nx\n",
    "import igraph as ig\n",
    "import logging\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoderModel:\n",
    "    def __init__(self, dims, internal_act=None, output_act=None):\n",
    "        self.data = mx.symbol.Variable('data')\n",
    "        self.y = mx.symbol.Variable('label')\n",
    "        self.fc1_weight = mx.symbol.Variable('fc1_weight')\n",
    "        self.fc1_bias = mx.symbol.Variable('fc1_bias')\n",
    "        self.fc2_weight = mx.symbol.Variable('fc2_weight')\n",
    "        self.fc2_bias = mx.symbol.Variable('fc2_bias')\n",
    "        x = mx.symbol.FullyConnected(data=self.data, weight=self.fc1_weight,\n",
    "                                     bias=self.fc1_bias, num_hidden=dims[1])\n",
    "        if (internal_act is not None):\n",
    "            x = mx.symbol.Activation(data=x, act_type=internal_act)\n",
    "            print(\"Internal activation: \" + internal_act)\n",
    "        self.layer1 = x\n",
    "        x = mx.symbol.FullyConnected(data=x, weight=self.fc2_weight,\n",
    "                                     bias=self.fc2_bias, num_hidden=dims[2])\n",
    "        if (output_act is not None):\n",
    "            x = mx.symbol.Activation(data=x, act_type=output_act)\n",
    "            print(\"Output activation: \" + output_act)\n",
    "        self.layer2 = x\n",
    "        # TODO How about using L1/L2 regularization.\n",
    "        self.loss = mx.symbol.LinearRegressionOutput(data=x, label=self.y)\n",
    "        self.model = mx.mod.Module(symbol=self.loss, data_names=['data'], label_names = ['label'])\n",
    "\n",
    "    def fit(self, data, batch_size, num_epoch, params=None, learning_rate=0.005, reinit_opt=True):\n",
    "        data_iter = mx.io.NDArrayIter(data={'data':data}, label={'label':data},\n",
    "                batch_size=batch_size, shuffle=True,\n",
    "                last_batch_handle='roll_over')\n",
    "        \n",
    "        if (params is None):\n",
    "            print(\"Learning rate: \" + str(learning_rate))\n",
    "            print(\"batch size: \" + str(batch_size))\n",
    "            print(\"internal #epochs: \" + str(num_epoch))\n",
    "            # allocate memory given the input data and label shapes\n",
    "            self.model.bind(data_shapes=data_iter.provide_data, label_shapes=data_iter.provide_label)\n",
    "            # initialize parameters by uniform random numbers\n",
    "            self.model.init_params(initializer=mx.init.Uniform(scale=.1))\n",
    "            # use SGD with learning rate 0.1 to train\n",
    "            self.model.init_optimizer(optimizer='sgd',\n",
    "                                      optimizer_params={'learning_rate': learning_rate,\n",
    "                                                        'momentum': 0.9})\n",
    "        else:\n",
    "            self.model.set_params(arg_params=params, aux_params=None, force_init=True)\n",
    "            if (reinit_opt):\n",
    "                print(\"reinit optimizer. New learning rate: \" + str(learning_rate))\n",
    "                self.model.init_optimizer(optimizer='sgd',\n",
    "                                          optimizer_params={'learning_rate': learning_rate,\n",
    "                                                            'momentum': 0.9}, force_init=True)\n",
    "        # use accuracy as the metric\n",
    "        metric = mx.metric.create('acc')\n",
    "        # train 5 epochs, i.e. going over the data iter one pass\n",
    "        for epoch in range(num_epoch):\n",
    "            data_iter.reset()\n",
    "            metric.reset()\n",
    "            for batch in data_iter:\n",
    "                self.model.forward(batch, is_train=True)       # compute predictions\n",
    "                self.model.update_metric(metric, batch.label)  # accumulate prediction accuracy\n",
    "                self.model.backward()                          # compute gradients\n",
    "                self.model.update()                            # update parameters\n",
    "            #print('Epoch %d, Training %s' % (epoch, metric.get()))\n",
    "        #self.model.fit(data_iter, optimizer_params={'learning_rate':learning_rate, 'momentum': 0.9},\n",
    "        #        optimizer='sgd', num_epoch=50, eval_metric='mse', force_rebind=True,\n",
    "        #        batch_end_callback = mx.callback.Speedometer(batch_size, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_act(act):\n",
    "    if (act == 'sigmoid'):\n",
    "        return sp.special.expit\n",
    "    elif (act == 'tanh'):\n",
    "        return np.tanh\n",
    "    elif (act == 'relu'):\n",
    "        return lambda x: np.maximum(x, 0)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def train(data, num_dims, num_epoc, internal_act=None, output_act=None, learning_rate=0.005, batch_size=50):\n",
    "    int_epoc = 100\n",
    "    params = None\n",
    "    model = AutoEncoderModel([data.shape[1], num_dims, data.shape[1]],\n",
    "                             internal_act, output_act)\n",
    "    prev_val = None\n",
    "    reinit_opt = True\n",
    "    for i in range(num_epoc/int_epoc):\n",
    "        curr = time.time()\n",
    "        model.fit(data, batch_size, int_epoc, params, learning_rate, reinit_opt=reinit_opt)\n",
    "        print(str(int_epoc) + \" epochs takes \" + str(time.time() - curr) + \" seconds\")\n",
    "        reinit_opt = False\n",
    "\n",
    "        params = model.model.get_params()[0]\n",
    "        fc1_weight = params.get('fc1_weight').asnumpy()\n",
    "        fc1_bias = params.get('fc1_bias').asnumpy()\n",
    "        fc2_weight = params.get('fc2_weight').asnumpy()\n",
    "        fc2_bias = params.get('fc2_bias').asnumpy()\n",
    "\n",
    "        np_data = data.asnumpy()\n",
    "        hidden = np.dot(np_data, fc1_weight.T) + fc1_bias\n",
    "        act_func = get_act(internal_act)\n",
    "        if (act_func is not None):\n",
    "            hidden = act_func(hidden)\n",
    "        output = np.dot(hidden, fc2_weight.T) + fc2_bias\n",
    "        act_func = get_act(output_act)\n",
    "        if (act_func is not None):\n",
    "            output = act_func(output)\n",
    "        val = np.sum(np.square(output - np_data))\n",
    "        print(\"epoc \" + str((i + 1) * int_epoc) + \": \" + str(val))\n",
    "        if (prev_val is not None and prev_val < val):\n",
    "            learning_rate = learning_rate / 2\n",
    "            reinit_opt = True\n",
    "        prev_val = val\n",
    "        sys.stdout.flush()\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run on a low-rank data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max: \n",
      "[ 5.70393372]\n",
      "<NDArray 1 @cpu(0)>\n",
      "(1000L, 100L)\n"
     ]
    }
   ],
   "source": [
    "rand_data1 = mx.ndarray.random_uniform(shape=[1000, 10])\n",
    "rand_data2 = mx.ndarray.random_uniform(shape=[10, 100])\n",
    "rand_data = mx.ndarray.dot(rand_data1, rand_data2)\n",
    "print(\"max: \" + str(mx.ndarray.max(rand_data)))\n",
    "rand_data = rand_data / mx.ndarray.max(rand_data)\n",
    "print(rand_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.27671653  0.11496934  0.22401108 ..., -0.1150435   0.24723341\n",
      "   4.58532953]\n",
      " [-0.05978573 -0.19632138 -0.02645103 ..., -0.28306383 -0.10512527\n",
      "   5.29204845]\n",
      " [ 0.07837401  0.06769264  0.27338678 ...,  0.01698423  0.0203772\n",
      "   3.47058678]\n",
      " ..., \n",
      " [ 0.02188545  0.09232116  0.26843369 ...,  0.08963827  0.07680167\n",
      "   3.58912826]\n",
      " [ 0.15893728 -0.1496805   0.11094719 ...,  0.2932995  -0.05907928\n",
      "   4.64020538]\n",
      " [ 0.03004075  0.03486993  0.02080921 ...,  0.05377063  0.13145779\n",
      "   4.39051485]]\n",
      "4984.47589737\n",
      "-528.592736117\n",
      "svd error: 5.74806e-09\n"
     ]
    }
   ],
   "source": [
    "np_rand_data = rand_data.asnumpy()\n",
    "U, s, Vh = sp.sparse.linalg.svds(np_rand_data, k=10)\n",
    "low_dim_data = np.dot(np_rand_data, Vh.T)\n",
    "print(low_dim_data)\n",
    "print(sum(low_dim_data[low_dim_data > 0]))\n",
    "print(sum(low_dim_data[low_dim_data < 0]))\n",
    "res = np.dot(low_dim_data, Vh)\n",
    "print(\"svd error: \" + str(np.sum(np.square(res - np_rand_data))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.2\n",
      "batch size: 50\n",
      "internal #epochs: 100\n",
      "100 epochs takes 2.23493695259 seconds\n",
      "epoc 0: 173.404\n",
      "100 epochs takes 2.16981911659 seconds\n",
      "epoc 100: 127.83\n",
      "100 epochs takes 2.17004418373 seconds\n",
      "epoc 200: 81.2349\n",
      "100 epochs takes 2.26216101646 seconds\n",
      "epoc 300: 49.3242\n",
      "100 epochs takes 2.21550703049 seconds\n",
      "epoc 400: 31.235\n",
      "100 epochs takes 2.20563292503 seconds\n",
      "epoc 500: 19.9435\n",
      "100 epochs takes 2.22947406769 seconds\n",
      "epoc 600: 13.2789\n",
      "100 epochs takes 2.19789195061 seconds\n",
      "epoc 700: 9.33316\n",
      "100 epochs takes 2.29411387444 seconds\n",
      "epoc 800: 6.16517\n",
      "100 epochs takes 2.25340390205 seconds\n",
      "epoc 900: 3.44699\n",
      "100 epochs takes 2.25359797478 seconds\n",
      "epoc 1000: 1.62342\n",
      "100 epochs takes 2.22027087212 seconds\n",
      "epoc 1100: 0.696142\n",
      "100 epochs takes 2.18843913078 seconds\n",
      "epoc 1200: 0.294522\n",
      "100 epochs takes 2.2648499012 seconds\n",
      "epoc 1300: 0.128039\n",
      "100 epochs takes 2.2517490387 seconds\n",
      "epoc 1400: 0.0572501\n",
      "100 epochs takes 2.23173594475 seconds\n",
      "epoc 1500: 0.026083\n",
      "100 epochs takes 2.27710700035 seconds\n",
      "epoc 1600: 0.0120273\n",
      "100 epochs takes 2.25829005241 seconds\n",
      "epoc 1700: 0.00557823\n",
      "100 epochs takes 2.27507710457 seconds\n",
      "epoc 1800: 0.00259214\n",
      "100 epochs takes 2.31578803062 seconds\n",
      "epoc 1900: 0.00120796\n",
      "100 epochs takes 2.33490896225 seconds\n",
      "epoc 2000: 0.000562821\n",
      "100 epochs takes 2.24649000168 seconds\n",
      "epoc 2100: 0.000262905\n",
      "100 epochs takes 2.30915403366 seconds\n",
      "epoc 2200: 0.000122799\n",
      "100 epochs takes 2.26845407486 seconds\n",
      "epoc 2300: 5.75998e-05\n",
      "100 epochs takes 2.21849393845 seconds\n",
      "epoc 2400: 2.72296e-05\n",
      "100 epochs takes 2.2021920681 seconds\n",
      "epoc 2500: 1.30932e-05\n",
      "100 epochs takes 2.16719007492 seconds\n",
      "epoc 2600: 6.52116e-06\n",
      "100 epochs takes 2.2364590168 seconds\n",
      "epoc 2700: 3.65042e-06\n",
      "100 epochs takes 2.20147585869 seconds\n",
      "epoc 2800: 2.42682e-06\n",
      "100 epochs takes 2.16406702995 seconds\n",
      "epoc 2900: 1.97019e-06\n",
      "100 epochs takes 2.13574504852 seconds\n",
      "epoc 3000: 1.73175e-06\n",
      "100 epochs takes 2.22770595551 seconds\n",
      "epoc 3100: 1.62906e-06\n",
      "100 epochs takes 2.1506319046 seconds\n",
      "epoc 3200: 1.40719e-06\n",
      "100 epochs takes 2.62897205353 seconds\n",
      "epoc 3300: 1.38036e-06\n",
      "100 epochs takes 2.25553011894 seconds\n",
      "epoc 3400: 1.34314e-06\n",
      "100 epochs takes 2.1972219944 seconds\n",
      "epoc 3500: 1.32213e-06\n",
      "100 epochs takes 2.2850561142 seconds\n",
      "epoc 3600: 1.30818e-06\n",
      "100 epochs takes 2.25021505356 seconds\n",
      "epoc 3700: 1.28412e-06\n",
      "100 epochs takes 2.23188900948 seconds\n",
      "epoc 3800: 1.27022e-06\n",
      "100 epochs takes 2.18267893791 seconds\n",
      "epoc 3900: 1.24969e-06\n",
      "100 epochs takes 2.30669784546 seconds\n",
      "epoc 4000: 1.24176e-06\n",
      "100 epochs takes 2.27865886688 seconds\n",
      "epoc 4100: 1.22098e-06\n",
      "100 epochs takes 2.19400501251 seconds\n",
      "epoc 4200: 1.21351e-06\n",
      "100 epochs takes 2.16883611679 seconds\n",
      "epoc 4300: 1.20416e-06\n",
      "100 epochs takes 2.25559592247 seconds\n",
      "epoc 4400: 1.19826e-06\n",
      "100 epochs takes 2.2708439827 seconds\n",
      "epoc 4500: 1.18175e-06\n",
      "100 epochs takes 2.23256206512 seconds\n",
      "epoc 4600: 1.17711e-06\n",
      "100 epochs takes 2.16053509712 seconds\n",
      "epoc 4700: 1.1395e-06\n",
      "100 epochs takes 2.17243790627 seconds\n",
      "epoc 4800: 1.13596e-06\n",
      "100 epochs takes 2.17402005196 seconds\n",
      "epoc 4900: 1.12883e-06\n"
     ]
    }
   ],
   "source": [
    "params_linear_r10=train(rand_data, 10, 5000, learning_rate=0.2, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Internal activation: tanh\n",
      "Learning rate: 0.4\n",
      "batch size: 100\n",
      "internal #epochs: 100\n",
      "100 epochs takes 1.3073489666 seconds\n",
      "epoc 0: 184.826\n",
      "100 epochs takes 1.33411216736 seconds\n",
      "epoc 100: 151.107\n",
      "100 epochs takes 1.3682179451 seconds\n",
      "epoc 200: 104.392\n",
      "100 epochs takes 1.34697604179 seconds\n",
      "epoc 300: 66.445\n",
      "100 epochs takes 1.3471288681 seconds\n",
      "epoc 400: 42.5984\n",
      "100 epochs takes 1.27479887009 seconds\n",
      "epoc 500: 30.4482\n",
      "100 epochs takes 1.34358310699 seconds\n",
      "epoc 600: 23.3595\n",
      "100 epochs takes 1.2657148838 seconds\n",
      "epoc 700: 17.126\n",
      "100 epochs takes 1.33024120331 seconds\n",
      "epoc 800: 12.4677\n",
      "100 epochs takes 1.3495721817 seconds\n",
      "epoc 900: 9.29682\n",
      "100 epochs takes 1.42844581604 seconds\n",
      "epoc 1000: 6.61072\n",
      "100 epochs takes 1.3458750248 seconds\n",
      "epoc 1100: 4.28825\n",
      "100 epochs takes 1.30867600441 seconds\n",
      "epoc 1200: 2.67795\n",
      "100 epochs takes 1.3676970005 seconds\n",
      "epoc 1300: 1.80303\n",
      "100 epochs takes 1.32587099075 seconds\n",
      "epoc 1400: 1.38844\n",
      "100 epochs takes 1.3152859211 seconds\n",
      "epoc 1500: 1.18517\n",
      "100 epochs takes 1.23243784904 seconds\n",
      "epoc 1600: 1.09658\n",
      "100 epochs takes 1.2149078846 seconds\n",
      "epoc 1700: 1.02094\n",
      "100 epochs takes 1.34785294533 seconds\n",
      "epoc 1800: 0.96739\n",
      "100 epochs takes 1.27730989456 seconds\n",
      "epoc 1900: 0.949986\n",
      "100 epochs takes 1.27659010887 seconds\n",
      "epoc 2000: 0.9102\n",
      "100 epochs takes 1.29900503159 seconds\n",
      "epoc 2100: 0.8789\n",
      "100 epochs takes 1.29998707771 seconds\n",
      "epoc 2200: 0.852123\n",
      "100 epochs takes 1.27945899963 seconds\n",
      "epoc 2300: 0.840555\n",
      "100 epochs takes 1.32416200638 seconds\n",
      "epoc 2400: 0.82246\n",
      "100 epochs takes 1.30071806908 seconds\n",
      "epoc 2500: 0.789134\n",
      "100 epochs takes 1.26489496231 seconds\n",
      "epoc 2600: 0.771803\n",
      "100 epochs takes 1.32125496864 seconds\n",
      "epoc 2700: 0.753659\n",
      "100 epochs takes 1.30523705482 seconds\n",
      "epoc 2800: 0.736314\n",
      "100 epochs takes 1.25992584229 seconds\n",
      "epoc 2900: 0.720386\n"
     ]
    }
   ],
   "source": [
    "params_sigmoid_r10=train(rand_data, 10, 5000, internal_act='tanh', learning_rate=0.4, batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run on real data\n",
    "\n",
    "We compute the embedding on a graph with 81306 vertices and 1768149 vertices. To embed the graph into 10 dimensions, we start with the most densest columns and increase the number of columns to embed. When we increase the number of columns to embed, we use the parameters trained from the previous run (on the dataset with a smaller number of columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "elg = nx.read_edgelist(\"/home/ubuntu/datasets/twitter_combined.txt\")\n",
    "spm = nx.to_scipy_sparse_matrix(elg, dtype='f')\n",
    "colsum = np.ravel(spm.sum(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the embedding on the densest 10 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ 2490.  3383.  2484.  2758.  2476.  1789.  2133.  3011.  3239.  2155.]\n",
      "<NDArray 10 @cpu(0)>\n",
      "(81306L, 10L)\n"
     ]
    }
   ],
   "source": [
    "max10 = np.sort(np.ravel(colsum), axis=None)[len(colsum) - 10]\n",
    "data10 = mx.ndarray.sparse.csr_matrix(spm[:,colsum >= max10])\n",
    "print(mx.ndarray.sum(data10, axis=0))\n",
    "print(data10.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(81306, 10)\n",
      "1.21314\n",
      "-2.3992\n",
      "svd error: 6.78263e-09\n"
     ]
    }
   ],
   "source": [
    "np_data10 = data10.asnumpy()\n",
    "U, s, Vh = sp.linalg.svd(np_data10, full_matrices=False)\n",
    "low_dim_data = np.dot(np_data10, Vh.T)\n",
    "print(low_dim_data.shape)\n",
    "print(np.max(low_dim_data))\n",
    "print(np.min(low_dim_data))\n",
    "res = np.dot(low_dim_data, Vh)\n",
    "print(\"svd error: \" + str(np.sum(np.square(res - np_data10))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.1\n",
      "batch size: 2000\n",
      "internal #epochs: 100\n",
      "100 epochs takes 6.57384419441 seconds\n",
      "epoc 0: 661.281\n",
      "100 epochs takes 7.22010612488 seconds\n",
      "epoc 100: 205.035\n",
      "100 epochs takes 6.69524002075 seconds\n",
      "epoc 200: 86.0281\n",
      "100 epochs takes 6.86633992195 seconds\n",
      "epoc 300: 69.2754\n",
      "100 epochs takes 7.26551198959 seconds\n",
      "epoc 400: 48.0851\n",
      "100 epochs takes 7.08142089844 seconds\n",
      "epoc 500: 24.3964\n",
      "100 epochs takes 6.72911000252 seconds\n",
      "epoc 600: 8.38904\n",
      "100 epochs takes 6.66133213043 seconds\n",
      "epoc 700: 2.06232\n",
      "100 epochs takes 6.76626992226 seconds\n",
      "epoc 800: 0.410246\n",
      "100 epochs takes 7.24766898155 seconds\n",
      "epoc 900: 0.0735736\n",
      "100 epochs takes 6.81751203537 seconds\n",
      "epoc 1000: 0.0125639\n",
      "100 epochs takes 7.10305809975 seconds\n",
      "epoc 1100: 0.00211213\n",
      "100 epochs takes 7.08962607384 seconds\n",
      "epoc 1200: 0.00035564\n",
      "100 epochs takes 6.72433185577 seconds\n",
      "epoc 1300: 6.57533e-05\n",
      "100 epochs takes 6.78700900078 seconds\n",
      "epoc 1400: 1.69345e-05\n",
      "100 epochs takes 6.82878088951 seconds\n",
      "epoc 1500: 1.13987e-05\n",
      "100 epochs takes 6.91550898552 seconds\n",
      "epoc 1600: 1.07326e-05\n",
      "100 epochs takes 6.56485104561 seconds\n",
      "epoc 1700: 1.04272e-05\n",
      "100 epochs takes 6.64296412468 seconds\n",
      "epoc 1800: 1.04248e-05\n",
      "100 epochs takes 7.0324318409 seconds\n",
      "epoc 1900: 1.0213e-05\n"
     ]
    }
   ],
   "source": [
    "params_linear10=train(data10, 10, 2000, internal_act=None, learning_rate=0.1, batch_size=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Internal activation: tanh\n",
      "Learning rate: 0.1\n",
      "batch size: 2000\n",
      "internal #epochs: 100\n",
      "100 epochs takes 8.79215788841 seconds\n",
      "epoc 0: 782.973\n",
      "100 epochs takes 8.54049897194 seconds\n",
      "epoc 100: 365.227\n",
      "100 epochs takes 8.9551320076 seconds\n",
      "epoc 200: 177.58\n",
      "100 epochs takes 8.56133294106 seconds\n",
      "epoc 300: 121.924\n",
      "100 epochs takes 8.85677790642 seconds\n",
      "epoc 400: 115.84\n",
      "100 epochs takes 8.66231513023 seconds\n",
      "epoc 500: 110.476\n",
      "100 epochs takes 8.81901407242 seconds\n",
      "epoc 600: 102.967\n",
      "100 epochs takes 9.03215098381 seconds\n",
      "epoc 700: 91.0163\n",
      "100 epochs takes 8.80674505234 seconds\n",
      "epoc 800: 72.9704\n",
      "100 epochs takes 8.79361891747 seconds\n",
      "epoc 900: 51.2826\n",
      "100 epochs takes 8.95192408562 seconds\n",
      "epoc 1000: 33.1414\n",
      "100 epochs takes 8.9709789753 seconds\n",
      "epoc 1100: 22.8834\n",
      "100 epochs takes 8.84673404694 seconds\n",
      "epoc 1200: 18.4841\n",
      "100 epochs takes 9.01217198372 seconds\n",
      "epoc 1300: 16.636\n",
      "100 epochs takes 8.86660695076 seconds\n",
      "epoc 1400: 15.6374\n",
      "100 epochs takes 8.89191913605 seconds\n",
      "epoc 1500: 14.9088\n",
      "100 epochs takes 9.16901397705 seconds\n",
      "epoc 1600: 14.2892\n",
      "100 epochs takes 9.02341890335 seconds\n",
      "epoc 1700: 13.7308\n",
      "100 epochs takes 9.12369418144 seconds\n",
      "epoc 1800: 13.2201\n",
      "100 epochs takes 9.03474807739 seconds\n",
      "epoc 1900: 12.7497\n",
      "100 epochs takes 9.10241293907 seconds\n",
      "epoc 2000: 12.3147\n",
      "100 epochs takes 8.91687321663 seconds\n",
      "epoc 2100: 11.9099\n",
      "100 epochs takes 8.86173796654 seconds\n",
      "epoc 2200: 11.5329\n",
      "100 epochs takes 8.76117992401 seconds\n",
      "epoc 2300: 11.1801\n",
      "100 epochs takes 8.98178315163 seconds\n",
      "epoc 2400: 10.8497\n",
      "100 epochs takes 8.97622990608 seconds\n",
      "epoc 2500: 10.5398\n",
      "100 epochs takes 8.96583509445 seconds\n",
      "epoc 2600: 10.2481\n",
      "100 epochs takes 8.74402785301 seconds\n",
      "epoc 2700: 9.97309\n",
      "100 epochs takes 8.94157505035 seconds\n",
      "epoc 2800: 9.71337\n",
      "100 epochs takes 8.92731595039 seconds\n",
      "epoc 2900: 9.46753\n",
      "100 epochs takes 9.16012406349 seconds\n",
      "epoc 3000: 9.23465\n",
      "100 epochs takes 9.10220098495 seconds\n",
      "epoc 3100: 9.0137\n",
      "100 epochs takes 8.79331207275 seconds\n",
      "epoc 3200: 8.80388\n",
      "100 epochs takes 9.00299715996 seconds\n",
      "epoc 3300: 8.60433\n",
      "100 epochs takes 9.25800800323 seconds\n",
      "epoc 3400: 8.41343\n",
      "100 epochs takes 9.14868998528 seconds\n",
      "epoc 3500: 8.23195\n",
      "100 epochs takes 9.17549395561 seconds\n",
      "epoc 3600: 8.05855\n",
      "100 epochs takes 8.9803211689 seconds\n",
      "epoc 3700: 7.89243\n",
      "100 epochs takes 8.9548740387 seconds\n",
      "epoc 3800: 7.73379\n",
      "100 epochs takes 8.80385994911 seconds\n",
      "epoc 3900: 7.58222\n",
      "100 epochs takes 8.8269507885 seconds\n",
      "epoc 4000: 7.43603\n",
      "100 epochs takes 8.99089717865 seconds\n",
      "epoc 4100: 7.29632\n",
      "100 epochs takes 9.08541297913 seconds\n",
      "epoc 4200: 7.16174\n",
      "100 epochs takes 9.28933691978 seconds\n",
      "epoc 4300: 7.03268\n",
      "100 epochs takes 8.96507096291 seconds\n",
      "epoc 4400: 6.909\n",
      "100 epochs takes 9.39710307121 seconds\n",
      "epoc 4500: 6.78875\n",
      "100 epochs takes 8.90583276749 seconds\n",
      "epoc 4600: 6.67336\n",
      "100 epochs takes 9.05141997337 seconds\n",
      "epoc 4700: 6.56218\n",
      "100 epochs takes 8.9608528614 seconds\n",
      "epoc 4800: 6.45487\n",
      "100 epochs takes 8.85131001472 seconds\n",
      "epoc 4900: 6.35104\n",
      "100 epochs takes 9.39655399323 seconds\n",
      "epoc 5000: 6.25144\n",
      "100 epochs takes 8.91017007828 seconds\n",
      "epoc 5100: 6.15445\n",
      "100 epochs takes 8.99461984634 seconds\n",
      "epoc 5200: 6.06074\n",
      "100 epochs takes 8.90814495087 seconds\n",
      "epoc 5300: 5.97022\n",
      "100 epochs takes 8.97487306595 seconds\n",
      "epoc 5400: 5.88268\n",
      "100 epochs takes 8.99152302742 seconds\n",
      "epoc 5500: 5.79765\n",
      "100 epochs takes 9.04438400269 seconds\n",
      "epoc 5600: 5.7155\n",
      "100 epochs takes 8.75118112564 seconds\n",
      "epoc 5700: 5.63531\n",
      "100 epochs takes 8.78787493706 seconds\n",
      "epoc 5800: 5.55775\n",
      "100 epochs takes 8.87168598175 seconds\n",
      "epoc 5900: 5.48237\n",
      "100 epochs takes 8.75983810425 seconds\n",
      "epoc 6000: 5.40955\n",
      "100 epochs takes 8.96203184128 seconds\n",
      "epoc 6100: 5.33841\n",
      "100 epochs takes 8.83202695847 seconds\n",
      "epoc 6200: 5.26926\n",
      "100 epochs takes 9.0251698494 seconds\n",
      "epoc 6300: 5.20243\n",
      "100 epochs takes 8.99802994728 seconds\n",
      "epoc 6400: 5.13684\n",
      "100 epochs takes 9.07689094543 seconds\n",
      "epoc 6500: 5.07333\n",
      "100 epochs takes 9.12474298477 seconds\n",
      "epoc 6600: 5.0115\n",
      "100 epochs takes 8.97813200951 seconds\n",
      "epoc 6700: 4.95084\n",
      "100 epochs takes 8.87551784515 seconds\n",
      "epoc 6800: 4.89225\n",
      "100 epochs takes 8.84768891335 seconds\n",
      "epoc 6900: 4.83498\n",
      "100 epochs takes 8.87118601799 seconds\n",
      "epoc 7000: 4.77906\n",
      "100 epochs takes 8.90713715553 seconds\n",
      "epoc 7100: 4.72478\n",
      "100 epochs takes 8.8008480072 seconds\n",
      "epoc 7200: 4.67155\n",
      "100 epochs takes 9.08423781395 seconds\n",
      "epoc 7300: 4.61976\n",
      "100 epochs takes 9.16846489906 seconds\n",
      "epoc 7400: 4.56901\n",
      "100 epochs takes 8.78410005569 seconds\n",
      "epoc 7500: 4.51946\n",
      "100 epochs takes 9.10500884056 seconds\n",
      "epoc 7600: 4.47121\n",
      "100 epochs takes 9.28625392914 seconds\n",
      "epoc 7700: 4.42387\n",
      "100 epochs takes 9.01584601402 seconds\n",
      "epoc 7800: 4.37808\n",
      "100 epochs takes 9.02753996849 seconds\n",
      "epoc 7900: 4.33277\n",
      "100 epochs takes 8.8073720932 seconds\n",
      "epoc 8000: 4.28853\n",
      "100 epochs takes 9.11186099052 seconds\n",
      "epoc 8100: 4.24542\n",
      "100 epochs takes 8.84149217606 seconds\n",
      "epoc 8200: 4.20317\n",
      "100 epochs takes 9.14153385162 seconds\n",
      "epoc 8300: 4.16191\n",
      "100 epochs takes 9.28389811516 seconds\n",
      "epoc 8400: 4.12135\n",
      "100 epochs takes 9.21483898163 seconds\n",
      "epoc 8500: 4.08176\n",
      "100 epochs takes 9.01534509659 seconds\n",
      "epoc 8600: 4.04291\n",
      "100 epochs takes 9.03191614151 seconds\n",
      "epoc 8700: 4.00554\n",
      "100 epochs takes 8.86402392387 seconds\n",
      "epoc 8800: 3.96768\n",
      "100 epochs takes 9.02093601227 seconds\n",
      "epoc 8900: 3.93112\n",
      "100 epochs takes 9.21815180779 seconds\n",
      "epoc 9000: 3.8953\n",
      "100 epochs takes 9.00927686691 seconds\n",
      "epoc 9100: 3.86031\n",
      "100 epochs takes 8.97904706001 seconds\n",
      "epoc 9200: 3.82586\n",
      "100 epochs takes 8.95766806602 seconds\n",
      "epoc 9300: 3.7921\n",
      "100 epochs takes 9.04226517677 seconds\n",
      "epoc 9400: 3.7591\n",
      "100 epochs takes 9.04830908775 seconds\n",
      "epoc 9500: 3.72662\n",
      "100 epochs takes 8.87337398529 seconds\n",
      "epoc 9600: 3.69463\n",
      "100 epochs takes 9.24768996239 seconds\n",
      "epoc 9700: 3.66333\n",
      "100 epochs takes 9.24598193169 seconds\n",
      "epoc 9800: 3.63265\n",
      "100 epochs takes 8.82478404045 seconds\n",
      "epoc 9900: 3.6024\n"
     ]
    }
   ],
   "source": [
    "params_sigmoid10=train(data10, 10, 10000, internal_act='tanh', learning_rate=0.1, batch_size=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Internal activation: tanh\n",
      "Output activation: sigmoid\n",
      "Learning rate: 0.2\n",
      "batch size: 2000\n",
      "internal #epochs: 100\n",
      "100 epochs takes 11.51684618 seconds\n",
      "epoc 0: 9111.85\n",
      "100 epochs takes 11.5236680508 seconds\n",
      "epoc 100: 2590.8\n",
      "100 epochs takes 11.5863921642 seconds\n",
      "epoc 200: 1321.07\n",
      "100 epochs takes 11.642277956 seconds\n",
      "epoc 300: 901.515\n",
      "100 epochs takes 11.6188299656 seconds\n",
      "epoc 400: 730.788\n",
      "100 epochs takes 11.588654995 seconds\n",
      "epoc 500: 637.645\n",
      "100 epochs takes 11.7546470165 seconds\n",
      "epoc 600: 576.195\n",
      "100 epochs takes 11.7699801922 seconds\n",
      "epoc 700: 528.786\n",
      "100 epochs takes 11.7428228855 seconds\n",
      "epoc 800: 484.707\n",
      "100 epochs takes 12.0463659763 seconds\n",
      "epoc 900: 430.7\n",
      "100 epochs takes 11.7655608654 seconds\n",
      "epoc 1000: 365.675\n",
      "100 epochs takes 11.9206221104 seconds\n",
      "epoc 1100: 311.159\n",
      "100 epochs takes 11.8484351635 seconds\n",
      "epoc 1200: 272.315\n",
      "100 epochs takes 11.7809638977 seconds\n",
      "epoc 1300: 245.158\n",
      "100 epochs takes 11.9608340263 seconds\n",
      "epoc 1400: 225.124\n",
      "100 epochs takes 11.8433690071 seconds\n",
      "epoc 1500: 209.313\n",
      "100 epochs takes 11.9539070129 seconds\n",
      "epoc 1600: 195.996\n",
      "100 epochs takes 11.8835258484 seconds\n",
      "epoc 1700: 184.137\n",
      "100 epochs takes 11.8467459679 seconds\n",
      "epoc 1800: 173.077\n",
      "100 epochs takes 11.715572834 seconds\n",
      "epoc 1900: 162.413\n",
      "100 epochs takes 11.7422690392 seconds\n",
      "epoc 2000: 151.917\n",
      "100 epochs takes 11.7390151024 seconds\n",
      "epoc 2100: 141.512\n",
      "100 epochs takes 11.7286472321 seconds\n",
      "epoc 2200: 131.242\n",
      "100 epochs takes 11.6341650486 seconds\n",
      "epoc 2300: 121.221\n",
      "100 epochs takes 11.7403988838 seconds\n",
      "epoc 2400: 111.652\n",
      "100 epochs takes 11.691838026 seconds\n",
      "epoc 2500: 102.802\n",
      "100 epochs takes 11.6715891361 seconds\n",
      "epoc 2600: 94.8775\n",
      "100 epochs takes 12.1928348541 seconds\n",
      "epoc 2700: 87.9295\n",
      "100 epochs takes 11.7677659988 seconds\n",
      "epoc 2800: 81.8828\n",
      "100 epochs takes 11.8544280529 seconds\n",
      "epoc 2900: 76.6225\n",
      "100 epochs takes 11.8493819237 seconds\n",
      "epoc 3000: 72.0271\n",
      "100 epochs takes 11.8296599388 seconds\n",
      "epoc 3100: 67.9852\n",
      "100 epochs takes 11.7292470932 seconds\n",
      "epoc 3200: 64.413\n",
      "100 epochs takes 11.6891329288 seconds\n",
      "epoc 3300: 61.2351\n",
      "100 epochs takes 11.8531649113 seconds\n",
      "epoc 3400: 58.3886\n",
      "100 epochs takes 12.0546710491 seconds\n",
      "epoc 3500: 55.8275\n",
      "100 epochs takes 11.9626011848 seconds\n",
      "epoc 3600: 53.5081\n",
      "100 epochs takes 11.9730899334 seconds\n",
      "epoc 3700: 51.3998\n",
      "100 epochs takes 11.9464480877 seconds\n",
      "epoc 3800: 49.4727\n",
      "100 epochs takes 11.8331229687 seconds\n",
      "epoc 3900: 47.7046\n",
      "100 epochs takes 11.8558030128 seconds\n",
      "epoc 4000: 46.0749\n",
      "100 epochs takes 11.9944179058 seconds\n",
      "epoc 4100: 44.5679\n",
      "100 epochs takes 11.9929759502 seconds\n",
      "epoc 4200: 43.1699\n",
      "100 epochs takes 12.0159552097 seconds\n",
      "epoc 4300: 41.8688\n",
      "100 epochs takes 11.991920948 seconds\n",
      "epoc 4400: 40.654\n",
      "100 epochs takes 11.9732170105 seconds\n",
      "epoc 4500: 39.5169\n",
      "100 epochs takes 11.9317910671 seconds\n",
      "epoc 4600: 38.4497\n",
      "100 epochs takes 12.0005638599 seconds\n",
      "epoc 4700: 37.445\n",
      "100 epochs takes 11.9755480289 seconds\n",
      "epoc 4800: 36.4985\n",
      "100 epochs takes 11.9955661297 seconds\n",
      "epoc 4900: 35.6032\n",
      "100 epochs takes 12.0089600086 seconds\n",
      "epoc 5000: 34.7567\n",
      "100 epochs takes 12.0615420341 seconds\n",
      "epoc 5100: 33.9536\n",
      "100 epochs takes 11.8873550892 seconds\n",
      "epoc 5200: 33.191\n",
      "100 epochs takes 11.8203430176 seconds\n",
      "epoc 5300: 32.4653\n",
      "100 epochs takes 11.9144730568 seconds\n",
      "epoc 5400: 31.7735\n",
      "100 epochs takes 11.9335768223 seconds\n",
      "epoc 5500: 31.1138\n",
      "100 epochs takes 11.8567769527 seconds\n",
      "epoc 5600: 30.4833\n",
      "100 epochs takes 12.0305149555 seconds\n",
      "epoc 5700: 29.8802\n",
      "100 epochs takes 12.0419919491 seconds\n",
      "epoc 5800: 29.3019\n",
      "100 epochs takes 12.0018701553 seconds\n",
      "epoc 5900: 28.7474\n",
      "100 epochs takes 12.0580499172 seconds\n",
      "epoc 6000: 28.2157\n",
      "100 epochs takes 12.1110620499 seconds\n",
      "epoc 6100: 27.7048\n",
      "100 epochs takes 11.9219121933 seconds\n",
      "epoc 6200: 27.213\n",
      "100 epochs takes 11.9234969616 seconds\n",
      "epoc 6300: 26.7398\n",
      "100 epochs takes 11.7684190273 seconds\n",
      "epoc 6400: 26.2839\n",
      "100 epochs takes 11.9424631596 seconds\n",
      "epoc 6500: 25.8441\n",
      "100 epochs takes 11.6554481983 seconds\n",
      "epoc 6600: 25.4198\n",
      "100 epochs takes 11.6210398674 seconds\n",
      "epoc 6700: 25.0099\n",
      "100 epochs takes 11.7195339203 seconds\n",
      "epoc 6800: 24.6139\n",
      "100 epochs takes 11.7544260025 seconds\n",
      "epoc 6900: 24.2312\n",
      "100 epochs takes 11.7879700661 seconds\n",
      "epoc 7000: 23.8601\n",
      "100 epochs takes 11.8099918365 seconds\n",
      "epoc 7100: 23.5008\n",
      "100 epochs takes 11.734937191 seconds\n",
      "epoc 7200: 23.1525\n",
      "100 epochs takes 11.6930429935 seconds\n",
      "epoc 7300: 22.815\n",
      "100 epochs takes 11.6788179874 seconds\n",
      "epoc 7400: 22.4872\n",
      "100 epochs takes 11.854170084 seconds\n",
      "epoc 7500: 22.1692\n",
      "100 epochs takes 11.8347308636 seconds\n",
      "epoc 7600: 21.8598\n",
      "100 epochs takes 11.8830230236 seconds\n",
      "epoc 7700: 21.5594\n",
      "100 epochs takes 11.8762848377 seconds\n",
      "epoc 7800: 21.2676\n",
      "100 epochs takes 11.7575900555 seconds\n",
      "epoc 7900: 20.9847\n",
      "100 epochs takes 11.6388039589 seconds\n",
      "epoc 8000: 20.7092\n",
      "100 epochs takes 11.7657740116 seconds\n",
      "epoc 8100: 20.4409\n",
      "100 epochs takes 11.7834579945 seconds\n",
      "epoc 8200: 20.1796\n",
      "100 epochs takes 11.8353819847 seconds\n",
      "epoc 8300: 19.9252\n",
      "100 epochs takes 11.8330609798 seconds\n",
      "epoc 8400: 19.6773\n",
      "100 epochs takes 11.9323680401 seconds\n",
      "epoc 8500: 19.4349\n",
      "100 epochs takes 11.8077509403 seconds\n",
      "epoc 8600: 19.1994\n",
      "100 epochs takes 11.8920209408 seconds\n",
      "epoc 8700: 18.9687\n",
      "100 epochs takes 11.7785620689 seconds\n",
      "epoc 8800: 18.7443\n",
      "100 epochs takes 11.7298018932 seconds\n",
      "epoc 8900: 18.5244\n",
      "100 epochs takes 11.7377970219 seconds\n",
      "epoc 9000: 18.3099\n",
      "100 epochs takes 11.8393571377 seconds\n",
      "epoc 9100: 18.0999\n",
      "100 epochs takes 11.7830262184 seconds\n",
      "epoc 9200: 17.8946\n",
      "100 epochs takes 11.758202076 seconds\n",
      "epoc 9300: 17.6943\n",
      "100 epochs takes 11.8239820004 seconds\n",
      "epoc 9400: 17.4976\n",
      "100 epochs takes 11.7874960899 seconds\n",
      "epoc 9500: 17.3061\n",
      "100 epochs takes 11.7503261566 seconds\n",
      "epoc 9600: 17.119\n",
      "100 epochs takes 11.8413128853 seconds\n",
      "epoc 9700: 16.936\n",
      "100 epochs takes 11.8964400291 seconds\n",
      "epoc 9800: 16.7567\n",
      "100 epochs takes 11.8337278366 seconds\n",
      "epoc 9900: 16.5815\n"
     ]
    }
   ],
   "source": [
    "params_sigmoid10=train(data10, 10, 10000, internal_act='tanh', output_act='sigmoid', learning_rate=0.2, batch_size=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the embedding on the densest 30 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max30 = np.sort(np.ravel(colsum), axis=None)[len(colsum) - 30]\n",
    "sp_data30 = spm[:,colsum >= max30]\n",
    "data30 = mx.ndarray.sparse.csr_matrix(sp_data30)\n",
    "print(mx.ndarray.sum(data30, axis=0))\n",
    "print(data30.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to start with SVD and see how well it performs.\n",
    "One question we need to address is **what is the advantage of autoencoder over SVD**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, s, Vh = sp.sparse.linalg.svds(sp_data30, k=10)\n",
    "res = np.dot(sp_data30.dot(Vh.T), Vh)\n",
    "print(\"svd error: \" + str(np.sum(np.square(res - sp_data30))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_linear30=train(data30, 5000, internal_act=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params_linear30=train(data30, 5000, internal_act='sigmoid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the embedding on the densest 1000 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max1000 = np.sort(np.ravel(colsum), axis=None)[len(colsum) - 1000]\n",
    "sp_data1000 = spm[:,colsum >= max1000]\n",
    "data1000 = mx.ndarray.sparse.csr_matrix(sp_data1000)\n",
    "print(mx.ndarray.sum(data1000, axis=0))\n",
    "print(data1000.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, s, Vh = sp.sparse.linalg.svds(sp_data1000, k=100)\n",
    "res = np.dot(sp_data1000.dot(Vh.T), Vh)\n",
    "print(\"svd error: \" + str(np.sum(np.square(res - sp_data1000))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_linear1000=train(data1000, num_dims=100, num_epoc=5000, internal_act=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pref_matrix = [[0.9, 0.1], [0.1, 0.9]]\n",
    "block_sizes = [70, 30]\n",
    "g = ig.Graph.SBM(100, pref_matrix, block_sizes, directed=True)\n",
    "sim_spm = g.get_adjacency()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
